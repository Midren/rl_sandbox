_target_: rl_sandbox.agents.DreamerV2

clip_rewards: identity
imagination_horizon: 15
batch_cluster_size: 50
layer_norm: false

world_model:
  _target_: rl_sandbox.agents.dreamer.world_model.WorldModel
  _partial_: true
  batch_cluster_size: ${..batch_cluster_size}
  latent_dim: 32
  latent_classes: 32
  rssm_dim: 200
  discount_loss_scale: 1.0
  kl_loss_scale: 2
  kl_loss_balancing: 0.8
  kl_free_nats: 1.00
  discrete_rssm: false
  decode_vit: false
  vit_l2_ratio: 0.5
  vit_img_size: 224
  encode_vit: false
  predict_discount: false
  layer_norm: ${..layer_norm}

actor:
  _target_: rl_sandbox.agents.dreamer.ac.ImaginativeActor
  _partial_: true
  # mixing of reinforce and maximizing value func
  # for dm_control it is zero in Dreamer (Atari 1)
  reinforce_fraction: null
  entropy_scale: 1e-5
  layer_norm: ${..layer_norm}

critic:
  _target_: rl_sandbox.agents.dreamer.ac.ImaginativeCritic
  _partial_: true
  discount_factor: 0.99
  update_interval: 100
  # [0-1], 1 means hard update
  soft_update_fraction: 1
  # Lambda parameter for trainin deeper multi-step prediction
  value_target_lambda: 0.95
  layer_norm: ${..layer_norm}

wm_optim:
  _target_: rl_sandbox.utils.optimizer.Optimizer
  _partial_: true
  lr_scheduler: null
  lr: 3e-4
  eps: 1e-5
  weight_decay: 1e-6
  clip: 100

actor_optim:
  _target_: rl_sandbox.utils.optimizer.Optimizer
  _partial_: true
  lr: 8e-5
  eps: 1e-5
  weight_decay: 1e-6
  clip: 100

critic_optim:
  _target_: rl_sandbox.utils.optimizer.Optimizer
  _partial_: true
  lr: 8e-5
  eps: 1e-5
  weight_decay: 1e-6
  clip: 100
